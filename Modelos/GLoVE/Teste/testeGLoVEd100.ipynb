{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Encontra diretorio atual\n",
    "atual_dir = os.getcwd()\n",
    "\n",
    "# Acessa arquivo pkl das noticias\n",
    "parent_dir = os.path.split(atual_dir)\n",
    "\n",
    "parent_dir = os.path.split(parent_dir[0])\n",
    "\n",
    "parent_dir = os.path.split(parent_dir[0])\n",
    "\n",
    "caminho_pkl = os.path.join(parent_dir[0], \"Pre-processamento\\\\noticias_pre_processadas_df.pkl\")\n",
    "\n",
    "# Carregar dataframe salvo em formato pickle\n",
    "df = pd.read_pickle(caminho_pkl)\n",
    "\n",
    "# Total de classes\n",
    "classes = df['label'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import spacy\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Habilita suporte do tqdm para os métodos de progressão do pandas (como progress_aplly)\n",
    "tqdm.pandas()\n",
    "\n",
    "# Carrega modelo de linguagem 'pt_core_news_lg' do spacy para processamento de texto em português\n",
    "# Desabilita os componentes 'parser' e 'ner', já que não são necessários para a lematização\n",
    "modelo_spacy_nlp = spacy.load(\"pt_core_news_lg\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def preprocess_data(df, coluna_texto):\n",
    "    \"\"\"\n",
    "    Realiza o pré-processamento dos dados de um texto em um Dataframe do Pandas.\n",
    "    Remove pontuação, números e palavras comuns (stop words), converte para minúsculas, remove \n",
    "    acentos e símbolos diversos, e aplica lematização.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove pontuação\n",
    "    print(\"Removendo pontuação...\")\n",
    "    traducao = str.maketrans('', '', string.punctuation)\n",
    "    df[coluna_texto] = df[coluna_texto].progress_apply(lambda x: x.translate(traducao))\n",
    "\n",
    "    # Remove números\n",
    "    print(\"Removendo números...\")\n",
    "    traducao = str.maketrans('', '', string.digits)\n",
    "    df[coluna_texto] = df[coluna_texto].progress_apply(lambda x: x.translate(traducao))\n",
    "\n",
    "    # Remove acentos e símbolos diversos\n",
    "    print(\"Removendo acentos e símbolos diversos...\")\n",
    "    def remove_acentos_e_simbolos(text):\n",
    "        try:\n",
    "            # Normaliza a string para a forma NFKD e mantém apenas caracteres que não são diacríticos\n",
    "            # nem combinam caracteres com diacríticos\n",
    "            return ''.join(c for c in unicodedata.normalize('NFKD', text) if not unicodedata.combining(c) and unicodedata.category(c) != 'Mn')\n",
    "        except TypeError:\n",
    "            # Se não for possível normalizar um caractere, retorna uma string vazia\n",
    "            return ''\n",
    "    df[coluna_texto] = df[coluna_texto].progress_apply(lambda x: remove_acentos_e_simbolos(x))\n",
    "    \n",
    "    # Converte para minúsculas\n",
    "    print(\"Convertendo para minúsculas...\")\n",
    "    df[coluna_texto] = df[coluna_texto].progress_apply(lambda x: x.lower())\n",
    "\n",
    "    # Lematização\n",
    "    print(\"Computando Lematização...\")\n",
    "    def lematizar_texto(doc):\n",
    "        return [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    n_chunks = 10  # Ajuste esse valor de acordo com o tamanho da base de dados e a memória disponível no sistema\n",
    "    chunks = np.array_split(df, n_chunks) # Divide o dataframe em várias partes\n",
    "\n",
    "    chunks_processados = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processando segmento {i + 1} de {n_chunks}\")\n",
    "        chunk_processado = chunk.copy() # Cria uma cópia para realizar o processamento\n",
    "        \n",
    "        # Aplica a função 'lematizar_texto' a cada documento processado pelo spaCy (usando 'spacy_nlp_model.pipe') e\n",
    "        # atribui os resultados (uma lista de palavras lematizadas) à coluna 'coluna_texto' do DataFrame 'chunks_processados'.\n",
    "        # O tqdm é utilizado para exibir uma barra de progresso durante o processamento dos documentos.\n",
    "        chunk_processado[coluna_texto] = [lematizar_texto(doc) for doc in tqdm(modelo_spacy_nlp.pipe(chunk[coluna_texto].astype(str), batch_size=100, disable=['parser', 'ner']), total=len(chunk[coluna_texto]))]\n",
    "\n",
    "        # Junta as partes em uma lista, para formar o dataframe final\n",
    "        chunks_processados.append(chunk_processado)\n",
    "\n",
    "    concatenated_df = pd.concat(chunks_processados) # Concatenar os DataFrames processados\n",
    "\n",
    "    df[coluna_texto] = concatenated_df[coluna_texto] # Atribuir a coluna 'texto' processada de volta ao dataframe original\n",
    "    \n",
    "    # Remover tokens com espaços vazios\n",
    "    print(\"Remover tokens com espaços vazios...\")\n",
    "    df['Texto'] = df['Texto'].progress_apply(lambda x: [token for token in x if token.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "# Encontra diretorio atual\n",
    "atual_dir = os.getcwd()\n",
    "\n",
    "# Acessa arquivo pkl das noticias\n",
    "parent_dir = os.path.split(atual_dir)\n",
    "\n",
    "parent_dir = os.path.split(parent_dir[0])\n",
    "\n",
    "parent_dir = os.path.split(parent_dir[0])\n",
    "\n",
    "caminho = os.path.join(parent_dir[0], \"Modelos\\\\GLoVE\\\\Treinamento\\\\tokenizer.pkl\")\n",
    "\n",
    "# Carrega o tokenizador de um arquivo\n",
    "with open(caminho, 'rb') as file:\n",
    "    tokenizer = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Carregar a lista 'sequences' de um arquivo usando pickle\n",
    "# Encontra diretorio atual\n",
    "atual_dir = os.getcwd()\n",
    "\n",
    "# Acessa arquivo pkl das noticias\n",
    "parent_dir = os.path.split(atual_dir)\n",
    "\n",
    "parent_dir = os.path.split(parent_dir[0])\n",
    "\n",
    "\n",
    "caminho_pkl = os.path.join(parent_dir[0], \"Word2Vec\\\\sequences.pickle\")\n",
    "\n",
    "with open(caminho_pkl, 'rb') as handle:\n",
    "    sequences = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sequence length:  173.9763956201759\n",
      "Maximum sequence length:  3835\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calcula o comprimento de cada sequência em 'sequences' e armazena os resultados na lista 'sequence_lengths'\n",
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "# Calcula a média dos comprimentos das sequências usando a função 'mean' do numpy\n",
    "mean_length = np.mean(sequence_lengths)\n",
    "\n",
    "# Obtém o comprimento máximo das sequências usando a função 'max' do Python\n",
    "max_length = max(sequence_lengths)\n",
    "\n",
    "# Imprime a média e o comprimento máximo das sequências\n",
    "print(\"Mean sequence length: \", mean_length)\n",
    "print(\"Maximum sequence length: \", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Calcula uma porcentagem de 95% dos comprimentos das sequências (preserva 95% das sequências)\n",
    "max_length = int(np.percentile(sequence_lengths, 95))\n",
    "\n",
    "# Realiza o preenchimento das sequências com base no novo 'max_length'\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Carregar a lista 'sequences' de um arquivo usando pickle\n",
    "# Encontra diretorio atual\n",
    "atual_dir = os.getcwd()\n",
    "\n",
    "# Acessa arquivo pkl das noticias\n",
    "parent_dir = os.path.split(atual_dir)\n",
    "\n",
    "parent_dir = os.path.split(parent_dir[0])\n",
    "\n",
    "caminho_modelo = os.path.join(parent_dir[0], \"GLoVE\\\\Treinamento\")\n",
    "\n",
    "# Carrega arquivos pertinentes ao treinamento da rede neural e X_test e y_test (usados na avaliação de resultados)\n",
    "model = load_model(caminho_modelo + '\\\\modelo_BiLSTM_glove6b100d.h5')\n",
    "X_test = np.load(caminho_modelo + \"\\\\X_test_BiLSTM_glove6b100d.npy\")\n",
    "y_test = np.load(caminho_modelo + \"\\\\y_test_BiLSTM_glove6b100d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo pontuação...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 988.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo números...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo acentos e símbolos diversos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1020.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertendo para minúsculas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 989.46it/s]\n",
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\Detector\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computando Lematização...\n",
      "Processando segmento 1 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 2 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 3 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 4 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 5 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 6 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 7 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 8 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 9 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 10 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remover tokens com espaços vazios...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "# Dataframe simples com uma notícia só para aplicar a função preprocess_data\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "value = \"\"\"Conselho de Segurança da ONU aprova resolução de cessar-fogo imediato em Gaza\n",
    "Texto aprovado estipula que a pausa aconteça durante o Ramadã, que começou em 10 de março e vai até 9 de abril. Documento é de cumprimento obrigatório, mas ONU não tem poder coercitivo. Secretário-geral da ONU, Antonio Guterres, pediu que o governo israelense acatasse a decisão do conselho.\n",
    "O Conselho de Segurança da Organização das Nações Unidas (ONU) aprovou nesta segunda-feira (25) uma resolução de cessar-fogo imediato na Faixa de Gaza.\n",
    "A resolução, feita por um grupo de dez países rotativos liderados por Moçambique, é a primeira que o conselho conseguiu aprovar sobre um cessar-fogo no território palestino.\n",
    "\n",
    "Israel promove uma ofensiva militar no território palestino desde que terroristas do Hamas invadiram o território israelense e mataram centenas de pessoas, em outubro de 2023.\n",
    "\n",
    "A aprovação, no entanto, não uma solução para a guerra. O desafio agora é garantir que os atores envolvidos nela - o governo de Israel e o grupo terrorista - cumpram as determinações exigidas no texto da ONU.\n",
    "\n",
    "Isso porque, embora as resoluções do Conselho de Segurança sejam juridicamente vinculativas, na prática acabam ignoradas por muitos países.\n",
    "\n",
    "O secretário-geral da ONU, Antonio Guterres, pediu que o governo israelense acatasse a decisão do conselho.\n",
    "\n",
    "Do que se trata\n",
    "O texto determina um cessar-fogo durante o mês do Ramadã, o período sagrado para os muçulmanos — que começou dia 10 e termina em 9 de abril—, mas pede que a trégua aumente até virar permanente.\n",
    "\n",
    "A resolução também pede a \"libertação imediata e incondicional de reféns\" e “a necessidade urgente de expandir o fluxo” de ajuda humanitária para Gaza.\n",
    "O Conselho de Segurança é formado por 15 países: cinco com assento permanente (China, França, Rússia, Reino Unido e Estados Unidos) e dez rotativos (Argélia, Equador, Guiana, Japão, Malta, Moçambique, Coréia do Sul, Serra Leoa, Eslovênia e Suíça).\n",
    "\n",
    "Na semana passada, uma resolução dos EUA pedindo a pausa nos bombardeios foi vetada pela China e pela Rússia, que estão entre os cinco membros permanentes do Conselho de Segurança e, por isso, têm poder de veto.\n",
    "\n",
    "Proposta dos EUA vetada\n",
    "A proposta dos EUA, uma mudança de posição de Washington na guerra entre Israel e Hamas, previa o cessar-fogo imediato na guerra entre Israel e o grupo terrorista Hamas, além da libertação de reféns.\n",
    "\n",
    "Ao justificar seu voto, o embaixador da Rússia na ONU, Vassily Nebenzia, acusou os EUA de falsas promessas e de só reconhecer a necessidade de um cessar-fogo \"quando mais de 30 mil habitantes de Gaza já morreram\".\n",
    "\"\"\"\n",
    "\n",
    "# Crie um DataFrame com uma linha e a coluna 'data'\n",
    "df_predict = pd.DataFrame(data={'Texto': [value]})\n",
    "\n",
    "# Faz pré-processamento\n",
    "preprocess_data(df_predict, 'Texto')\n",
    "\n",
    "# Conversão dos dados para serem usados no modelo (rede neural)\n",
    "preprocessed_articles = df_predict['Texto'].tolist()\n",
    "\n",
    "sequences_test = []\n",
    "for text in tqdm(preprocessed_articles):\n",
    "    # Converte o texto em sequências de tokens usando o tokenizer\n",
    "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequences_test.append(sequence)\n",
    "    \n",
    "padded_example = pad_sequences(sequences_test, maxlen=max_length, padding='post')\n",
    "\n",
    "# Fazer a previsão usando o modelo\n",
    "predictions = model.predict(padded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14617221, 0.8538278 ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: 1\n"
     ]
    }
   ],
   "source": [
    "# Identificar a classe com a maior probabilidade\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "# Exibir a classe prevista\n",
    "print(f\"Classe prevista: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe original: Real\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Encontra diretorio atual\n",
    "atual_dir = os.getcwd()\n",
    "\n",
    "# Acessa arquivo csv das noticias\n",
    "parent_dir = os.path.split(atual_dir)\n",
    "\n",
    "parent_dir = os.path.split(parent_dir[0])\n",
    "\n",
    "parent_dir = os.path.split(parent_dir[0])\n",
    "\n",
    "caminho_csv = os.path.join(parent_dir[0], \"Pre-processamento\\\\noticias_dados_limpos.csv\")\n",
    "\n",
    "# Carregar dataframe salvo em formato csv\n",
    "df = pd.read_csv(caminho_csv)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Cria um objeto LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Transforma os labels para variáveis categóricas\n",
    "df['label'] = le.fit_transform(df['Categoria'])\n",
    "\n",
    "original_class = le.inverse_transform([predicted_class]) \n",
    "print(f\"Classe original: {original_class[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo pontuação...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 998.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo números...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removendo acentos e símbolos diversos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 978.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertendo para minúsculas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\Detector\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computando Lematização...\n",
      "Processando segmento 1 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 32.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 2 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 3 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 4 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 5 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 6 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 7 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 8 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 9 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando segmento 10 de 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remover tokens com espaços vazios...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 137ms/step\n"
     ]
    }
   ],
   "source": [
    "# Dataframe simples com uma notícia só para aplicar a função preprocess_data\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "value = \"\"\"\n",
    "\"Estudo Inovador Afirma que Ficar de Pijama o Dia Todo Aumenta a Produtividade\"\n",
    "\n",
    "Uma pesquisa recente realizada por uma equipe de psicólogos da Universidade de Bem-Estar Mental revelou uma descoberta surpreendente: ficar de pijama durante o dia pode aumentar significativamente a produtividade.\n",
    "\n",
    "Segundo os pesquisadores, vestir pijamas cria uma sensação de conforto e relaxamento, o que reduz os níveis de estresse e ansiedade. Isso, por sua vez, permite que as pessoas se concentrem mais em suas tarefas e sejam mais eficientes em seu trabalho.\n",
    "\n",
    "O estudo, que acompanhou mais de mil participantes ao longo de seis meses, descobriu que aqueles que adotaram a prática de trabalhar de pijama relataram uma melhoria notável em sua capacidade de concentração e tomada de decisões. Além disso, muitos participantes relataram sentir-se mais motivados e criativos em suas atividades diárias.\n",
    "\n",
    "Essa descoberta desafia a noção convencional de que vestir roupas formais é essencial para o sucesso profissional. Os pesquisadores agora estão explorando maneiras de integrar essa prática inovadora em ambientes de trabalho tradicionais, potencialmente revolucionando a cultura corporativa.\n",
    "\n",
    "Portanto, se você está procurando aumentar sua produtividade, talvez seja hora de abandonar o terno e adotar o conforto do pijama durante o expediente!\n",
    "\"\"\"\n",
    "\n",
    "# Crie um DataFrame com uma linha e a coluna 'data'\n",
    "df_predict = pd.DataFrame(data={'Texto': [value]})\n",
    "\n",
    "# Faz pré-processamento\n",
    "preprocess_data(df_predict, 'Texto')\n",
    "\n",
    "# Conversão dos dados para serem usados no modelo (rede neural)\n",
    "preprocessed_articles = df_predict['Texto'].tolist()\n",
    "\n",
    "sequences_test = []\n",
    "for text in tqdm(preprocessed_articles):\n",
    "    # Converte o texto em sequências de tokens usando o tokenizer\n",
    "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequences_test.append(sequence)\n",
    "    \n",
    "padded_example = pad_sequences(sequences_test, maxlen=max_length, padding='post')\n",
    "\n",
    "# Fazer a previsão usando o modelo\n",
    "predictions = model.predict(padded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9948832 , 0.00511682]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: 0\n"
     ]
    }
   ],
   "source": [
    "# Identificar a classe com a maior probabilidade\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "# Exibir a classe prevista\n",
    "print(f\"Classe prevista: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe original: Falso\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Encontra diretorio atual\n",
    "atual_dir = os.getcwd()\n",
    "\n",
    "# Acessa arquivo csv das noticias\n",
    "parent_dir = os.path.split(atual_dir)\n",
    "\n",
    "parent_dir = os.path.split(parent_dir[0])\n",
    "\n",
    "parent_dir = os.path.split(parent_dir[0])\n",
    "\n",
    "caminho_csv = os.path.join(parent_dir[0], \"Pre-processamento\\\\noticias_dados_limpos.csv\")\n",
    "\n",
    "# Carregar dataframe salvo em formato csv\n",
    "df = pd.read_csv(caminho_csv)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Cria um objeto LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Transforma os labels para variáveis categóricas\n",
    "df['label'] = le.fit_transform(df['Categoria'])\n",
    "\n",
    "original_class = le.inverse_transform([predicted_class]) \n",
    "print(f\"Classe original: {original_class[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
