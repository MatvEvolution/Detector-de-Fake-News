{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa a classe Word2Vec da biblioteca gensim, utilizada para criar e treinar modelos de embeddings de palavras com o algoritmo Word2Vec.\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Importa a classe CallbackAny2Vec da biblioteca gensim, que serve como base para criar callbacks personalizados durante o treinamento de modelos Word2Vec ou similares.\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.utils as ku\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define uma classe de callback personalizada que herda de CallbackAny2Vec.\n",
    "class SimpleTextCallback(CallbackAny2Vec):\n",
    "    def __init__(self, total_epochs):\n",
    "        self.epoch = 0  # Inicializa o contador de épocas (iterações de treinamento).\n",
    "        self.total_epochs = total_epochs  # Armazena o número total de épocas para exibição.\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1  # Incrementa o contador de épocas ao final de cada época.\n",
    "        print(f\"Época {self.epoch} de {self.total_epochs}...\")  # Imprime uma mensagem indicando a época atual e o total de épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega dataframe salvo em formato pickle\n",
    "df = pd.read_pickle(\"df_pre_processado.pkl\")\n",
    "\n",
    "# Converte a coluna 'Texto' do DataFrame em uma lista e armazena na variável preprocessed_articles, para ser usada no word2vec\n",
    "preprocessed_articles = df['Texto'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1 de 10...\n",
      "Época 2 de 10...\n",
      "Época 3 de 10...\n",
      "Época 4 de 10...\n",
      "Época 5 de 10...\n",
      "Época 6 de 10...\n",
      "Época 7 de 10...\n",
      "Época 8 de 10...\n",
      "Época 9 de 10...\n",
      "Época 10 de 10...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36699385, 37650100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instancia o modelo Word2Vec com os seguintes parâmetros:\n",
    "# vector_size=100: Dimensão do vetor de palavras gerado.\n",
    "# window=5: Tamanho da janela de contexto ao redor de cada palavra.\n",
    "# min_count=1: Ignora palavras com frequência total menor que 1.\n",
    "# workers=4: Número de threads a serem usadas para treinar o modelo.\n",
    "# Nota: Adicionar sg=0 altera o algoritmo para CBOW (Continuous Bag of Words).\n",
    "model_word2vec = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Constrói o vocabulário do modelo com base nos artigos pré-processados.\n",
    "model_word2vec.build_vocab(preprocessed_articles)\n",
    "\n",
    "# Define o número total de épocas para o treinamento do modelo.\n",
    "total_epochs = 10\n",
    "\n",
    "# Instancia o callback que exibe o progresso do treinamento após cada época.\n",
    "callback = SimpleTextCallback(total_epochs)\n",
    "\n",
    "# Treina o modelo Word2Vec nos artigos pré-processados, utilizando o número total de exemplos e épocas definidos.\n",
    "model_word2vec.train(preprocessed_articles, total_examples=model_word2vec.corpus_count, \n",
    "                     epochs=total_epochs, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o modelo Word2Vec para um arquivo\n",
    "model_word2vec.save(\"model_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('governar', 0.6748394966125488),\n",
       " ('Governo', 0.5884649157524109),\n",
       " ('gestoes', 0.5602782368659973),\n",
       " ('aliado', 0.5227554440498352),\n",
       " ('gestao', 0.5091262459754944),\n",
       " ('economica', 0.4790595471858978),\n",
       " ('intervencao', 0.46907633543014526),\n",
       " ('governador', 0.4503372311592102),\n",
       " ('plano', 0.44774729013442993),\n",
       " ('concessoes', 0.44446662068367004)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontra palavras mais similares de acordo com a palavra alvo\n",
    "model_word2vec.wv.most_similar('governo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria dois dicionários para mapear as palavras aos seus respectivos índices e vice-versa.\n",
    "# Isso é feito para facilitar a conversão entre palavras e índices durante a criação de sequências\n",
    "# numéricas que representam os textos. Essas sequências numéricas serão usadas como entrada para\n",
    "# modelos de aprendizado de máquina, como redes neurais.\n",
    "# Exemplo: \n",
    "# Suponha que o vocabulário seja ['casa', 'carro', 'jardim'], então os dicionários serão:\n",
    "# word_to_index = {'casa': 1, 'carro': 2, 'jardim': 3}\n",
    "# index_to_word = {1: 'casa', 2: 'carro', 3: 'jardim'}\n",
    "\n",
    "# 'word_to_index' é um dicionário que mapeia cada palavra ao seu índice correspondente.\n",
    "word_to_index = {}\n",
    "\n",
    "# 'index_to_word' é um dicionário que mapeia cada índice à palavra correspondente.\n",
    "index_to_word = {}\n",
    "\n",
    "# Itera sobre a lista de palavras únicas obtida do modelo Word2Vec\n",
    "for i, word in enumerate(model_word2vec.wv.index_to_key):\n",
    "    # Atribui a palavra ao índice i + 1 no dicionário 'word_to_index'.\n",
    "    # Os índices começam em 1 para reservar o índice 0 para preenchimento (padding) quando necessário.\n",
    "    word_to_index[word] = i + 1\n",
    "    \n",
    "    # Atribui o índice i + 1 à palavra no dicionário 'index_to_word'.\n",
    "    index_to_word[i + 1] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22284 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22284/22284 [00:01<00:00, 17888.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# 'sequences' é uma lista que armazenará as sequências numéricas correspondentes aos artigos pré-processados.\n",
    "sequences = []\n",
    "\n",
    "# Itera sobre os artigos pré-processados.\n",
    "for tokens in tqdm(preprocessed_articles):\n",
    "    # 'sequence' é uma lista temporária que armazenará a sequência numérica para o artigo atual.\n",
    "    sequence = []\n",
    "    \n",
    "    # Itera sobre os tokens (palavras) no artigo atual.\n",
    "    for token in tokens:\n",
    "        # Verifica se o token atual está presente no dicionário 'word_to_index'.\n",
    "        if token in word_to_index:\n",
    "            # Se o token estiver presente, adiciona o índice correspondente à lista 'sequence'.\n",
    "            sequence.append(word_to_index[token])\n",
    "    \n",
    "    # Após processar todos os tokens do artigo atual, adiciona a sequência numérica completa à lista 'sequences'.\n",
    "    sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poder decidir expulsar deputado federal Carlos gaguim partido Apos policia federal busca apreensoes gabinete de ele Camara legenda abrir espaco receber senadora expulsar pmdb katia abreu nota legenda informar afastamento parlamentar ja acordar filiado sigla “ parlamentar ter comunicar conclusao desfiliacao semana fato noticiar hoje executiva nacional poder solicitar imediato cancelamento filiacao quadro partido ” partido passado chegar cogitar lancar parlamentar candidato senado “ investigacao amplo apuracao eventual crime cometir consequente responsabilizacao envolvido ser puner maximo rigor lei independentemente posicao cargo ocupar ”\n"
     ]
    }
   ],
   "source": [
    "# Utiliza uma compreensão de lista para converter a sequência numérica do primeiro artigo em uma lista de palavras.\n",
    "# Para cada índice 'i' na sequência numérica 'sequences[0]', obtém a palavra correspondente no dicionário 'index_to_word'.\n",
    "# A compreensão de lista retorna uma lista de palavras.\n",
    "word_list = [index_to_word[i] for i in sequences[0]]\n",
    "\n",
    "# Usa o método 'join()' para combinar as palavras da lista 'word_list' em uma única string.\n",
    "# As palavras são separadas por um espaço em branco.\n",
    "text = \" \".join(word_list)\n",
    "\n",
    "# Imprime o texto reconstruído a partir da sequência numérica.\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sequence length:  168.95575300664154\n",
      "Maximum sequence length:  3835\n"
     ]
    }
   ],
   "source": [
    "# Calcula o comprimento de cada sequência em 'sequences' e armazena os resultados na lista 'sequence_lengths'\n",
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "# Calcula a média dos comprimentos das sequências usando a função 'mean' do numpy\n",
    "mean_length = np.mean(sequence_lengths)\n",
    "\n",
    "# Obtém o comprimento máximo das sequências usando a função 'max' do Python\n",
    "max_length = max(sequence_lengths)\n",
    "\n",
    "# Imprime a média e o comprimento máximo das sequências\n",
    "print(\"Mean sequence length: \", mean_length)\n",
    "print(\"Maximum sequence length: \", max_length)\n",
    "\n",
    "# Salva o valor de max_length em um arquivo pickle\n",
    "with open('max_length.pkl', 'wb') as f:\n",
    "    pickle.dump(max_length, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula 95% dos comprimentos das sequências (preserva 95% das sequências)\n",
    "max_length = int(np.percentile(sequence_lengths, 95))\n",
    "\n",
    "# Realiza o preenchimento das sequências com base no novo 'max_length'\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].nunique() # Retorna o número de categorias únicas (classes) presentes na coluna 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte rótulos para one-hot encoding\n",
    "num_classes = df['label'].nunique()\n",
    "labels = ku.to_categorical(df['label'], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva os rótulos (labels) em um arquivo pickle\n",
    "with open('labels.pkl', 'wb') as f:\n",
    "    pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide os dados em um conjunto de treino (70%) e um conjunto temporário (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(padded_sequences, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Divide o conjunto temporário em conjuntos de validação (15%) e teste (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva os dados divididos em arquivos pickle\n",
    "with open('train_data.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train, y_train), f)\n",
    "\n",
    "with open('val_data.pkl', 'wb') as f:\n",
    "    pickle.dump((X_val, y_val), f)\n",
    "\n",
    "with open('test_data.pkl', 'wb') as f:\n",
    "    pickle.dump((X_test, y_test), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
